{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in Data\n",
    "data = loadmat('MSdata.mat')\n",
    "X_train = data['trainx']\n",
    "y_train = data['trainy']\n",
    "X_test = data['testx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validated 5 folds:\n",
    "def create_folds(trainx, trainy, k=5):\n",
    "    folds_x, folds_y = [], []\n",
    "    n = len(trainx)\n",
    "    size = n//k\n",
    "    shuffled_idx = np.random.choice([i for i in range(n)], size = n, replace = False)\n",
    "    for i in range(k-1):\n",
    "        idx = shuffled_idx[i*size:(i+1)*size]\n",
    "        folds_x.append(trainx[idx])\n",
    "        folds_y.append(trainy[idx])\n",
    "    folds_x.append(trainx[i*size:])\n",
    "    folds_y.append(trainy[i*size:])\n",
    "    return [folds_x, folds_y]\n",
    "\n",
    "def create_data(folds, i=0):\n",
    "    x_test, y_test = folds[0][i], folds[1][i]\n",
    "    x_train = np.concatenate(folds[0][0:i] + folds[0][i+1:])\n",
    "    y_train = np.concatenate(folds[1][0:i] + folds[1][i+1:])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "folds = create_folds(X_train, y_train, k=5)\n",
    "x_train, y_train, x_test, y_test = create_data(folds, i=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch\n",
    "Custom dataset + Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDataset(Dataset):\n",
    "    def __init__(self, X_train, y_train, transform = None):\n",
    "        self.X = X_train\n",
    "        self.y = y_train\n",
    "        if transform:\n",
    "            self.X = transform(self.X)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]\n",
    "        if not self.y:\n",
    "            y = -1\n",
    "        else:\n",
    "            y = self.y[idx][0]\n",
    "        sample = {'x': torch.from_numpy(x).type(torch.float32),\n",
    "                  'y': torch.tensor(np.array([float(y)]))}\n",
    "        return sample\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SongDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=128,\n",
    "                              shuffle=True, \n",
    "                              num_workers=4)\n",
    "val_dataset = SongDataset(x_test, y_test)\n",
    "val_dataloader = DataLoader(val_dataset, \n",
    "                            batch_size=128,\n",
    "                            shuffle=False, \n",
    "                            num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SongDataset(X_test, None)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=128,\n",
    "                             shuffle=False, \n",
    "                             num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (fc_1): Linear(in_features=90, out_features=128, bias=True)\n",
      "  (norm1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (norm2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (norm3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_4): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 64, 3)\n",
    "#         self.norm1 = nn.BatchNorm2d(64)\n",
    "#         self.conv2 = nn.Conv2d(64, 32, 3)\n",
    "#         self.norm2 = nn.BatchNorm2d(32)\n",
    "        self.fc_1 = nn.Linear(90, 128)\n",
    "        self.norm1 = nn.BatchNorm1d(128)\n",
    "        self.fc_2 = nn.Linear(128, 64)\n",
    "        self.norm2 = nn.BatchNorm1d(64)\n",
    "        self.fc_3 = nn.Linear(64, 32)\n",
    "        self.norm3 = nn.BatchNorm1d(32)\n",
    "        self.fc_4 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = self.norm1(self.conv1(x))\n",
    "#         x = F.max_pool2d(F.relu(x), 2)\n",
    "#         x = self.norm2(self.conv2(x))\n",
    "#         x = F.max_pool2d(F.relu(x), 2)\n",
    "#         x = x.view(-1, 1)\n",
    "        x = self.norm1(self.fc_1(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.norm2(self.fc_2(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.norm3(self.fc_3(x))\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_4(x)\n",
    "        return x\n",
    "    \n",
    "model = DenseNet().cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [batch:   1000] loss: 6.23690\n",
      "  [batch:   2000] loss: 6.00955\n",
      "  [batch:   3000] loss: 5.98047\n",
      "Epoch: 1/50 Val Loss:6.009\n",
      "  [batch:   1000] loss: 5.93047\n",
      "  [batch:   2000] loss: 5.91786\n",
      "  [batch:   3000] loss: 5.90470\n",
      "Epoch: 2/50 Val Loss:5.966\n",
      "  [batch:   1000] loss: 5.88483\n",
      "  [batch:   2000] loss: 5.86239\n",
      "  [batch:   3000] loss: 5.85374\n",
      "Epoch: 3/50 Val Loss:5.916\n",
      "  [batch:   1000] loss: 5.81334\n",
      "  [batch:   2000] loss: 5.86747\n",
      "  [batch:   3000] loss: 5.85836\n",
      "Epoch: 4/50 Val Loss:6.081\n",
      "  [batch:   1000] loss: 5.88543\n",
      "  [batch:   2000] loss: 5.83171\n",
      "  [batch:   3000] loss: 5.87436\n",
      "Epoch: 5/50 Val Loss:5.883\n",
      "  [batch:   1000] loss: 5.82643\n",
      "  [batch:   2000] loss: 5.85711\n",
      "  [batch:   3000] loss: 5.82812\n",
      "Epoch: 6/50 Val Loss:5.914\n",
      "  [batch:   1000] loss: 5.79403\n",
      "  [batch:   2000] loss: 5.80010\n",
      "  [batch:   3000] loss: 5.86151\n",
      "Epoch: 7/50 Val Loss:6.116\n",
      "  [batch:   1000] loss: 5.84019\n",
      "  [batch:   2000] loss: 5.84209\n",
      "  [batch:   3000] loss: 5.82869\n",
      "Epoch: 8/50 Val Loss:5.895\n",
      "  [batch:   1000] loss: 5.85424\n",
      "  [batch:   2000] loss: 5.85731\n",
      "  [batch:   3000] loss: 5.80291\n",
      "Epoch: 9/50 Val Loss:5.996\n",
      "  [batch:   1000] loss: 5.85037\n",
      "  [batch:   2000] loss: 5.79389\n",
      "  [batch:   3000] loss: 5.84058\n",
      "Epoch: 10/50 Val Loss:6.055\n",
      "  [batch:   1000] loss: 5.79384\n",
      "  [batch:   2000] loss: 5.81520\n",
      "  [batch:   3000] loss: 5.79087\n",
      "Epoch: 11/50 Val Loss:6.222\n",
      "  [batch:   1000] loss: 5.82154\n",
      "  [batch:   2000] loss: 5.78276\n",
      "  [batch:   3000] loss: 5.85414\n",
      "Epoch: 12/50 Val Loss:5.833\n",
      "  [batch:   1000] loss: 5.77633\n",
      "  [batch:   2000] loss: 5.83657\n",
      "  [batch:   3000] loss: 5.80622\n",
      "Epoch: 13/50 Val Loss:5.829\n",
      "  [batch:   1000] loss: 5.77387\n",
      "  [batch:   2000] loss: 5.84896\n",
      "  [batch:   3000] loss: 5.83044\n",
      "Epoch: 14/50 Val Loss:5.858\n",
      "  [batch:   1000] loss: 5.84445\n",
      "  [batch:   2000] loss: 5.78946\n",
      "  [batch:   3000] loss: 5.80365\n",
      "Epoch: 15/50 Val Loss:5.836\n",
      "  [batch:   1000] loss: 5.79127\n",
      "  [batch:   2000] loss: 5.82530\n",
      "  [batch:   3000] loss: 5.81526\n",
      "Epoch: 16/50 Val Loss:5.868\n",
      "  [batch:   1000] loss: 5.80371\n",
      "  [batch:   2000] loss: 5.77998\n",
      "  [batch:   3000] loss: 5.80604\n",
      "Epoch: 17/50 Val Loss:5.85\n",
      "  [batch:   1000] loss: 5.75984\n",
      "  [batch:   2000] loss: 5.76896\n",
      "  [batch:   3000] loss: 5.81936\n",
      "Epoch: 18/50 Val Loss:5.925\n",
      "  [batch:   1000] loss: 5.81583\n",
      "  [batch:   2000] loss: 5.81242\n",
      "  [batch:   3000] loss: 5.81788\n",
      "Epoch: 19/50 Val Loss:5.848\n",
      "  [batch:   1000] loss: 5.78639\n",
      "  [batch:   2000] loss: 5.76667\n",
      "  [batch:   3000] loss: 5.84314\n",
      "Epoch: 20/50 Val Loss:6.026\n",
      "  [batch:   1000] loss: 5.77939\n",
      "  [batch:   2000] loss: 5.80039\n",
      "  [batch:   3000] loss: 5.82019\n",
      "Epoch: 21/50 Val Loss:5.841\n",
      "  [batch:   1000] loss: 5.79547\n",
      "  [batch:   2000] loss: 5.82133\n",
      "  [batch:   3000] loss: 5.77536\n",
      "Epoch: 22/50 Val Loss:6.108\n",
      "  [batch:   1000] loss: 5.75718\n",
      "  [batch:   2000] loss: 5.79031\n",
      "  [batch:   3000] loss: 5.78257\n",
      "Epoch: 23/50 Val Loss:5.845\n",
      "  [batch:   1000] loss: 5.78829\n",
      "  [batch:   2000] loss: 5.80696\n",
      "  [batch:   3000] loss: 5.80092\n",
      "Epoch: 24/50 Val Loss:5.835\n",
      "  [batch:   1000] loss: 5.76664\n",
      "  [batch:   2000] loss: 5.77777\n",
      "  [batch:   3000] loss: 5.80056\n",
      "Epoch: 25/50 Val Loss:5.856\n",
      "  [batch:   1000] loss: 5.77568\n",
      "  [batch:   2000] loss: 5.78042\n",
      "  [batch:   3000] loss: 5.80887\n",
      "Epoch: 26/50 Val Loss:5.893\n",
      "  [batch:   1000] loss: 5.76692\n",
      "  [batch:   2000] loss: 5.80144\n",
      "  [batch:   3000] loss: 5.76775\n",
      "Epoch: 27/50 Val Loss:6.045\n",
      "  [batch:   1000] loss: 5.75786\n",
      "  [batch:   2000] loss: 5.82244\n",
      "  [batch:   3000] loss: 5.78756\n",
      "Epoch: 28/50 Val Loss:5.854\n",
      "  [batch:   1000] loss: 5.77023\n",
      "  [batch:   2000] loss: 5.77575\n",
      "  [batch:   3000] loss: 5.79966\n",
      "Epoch: 29/50 Val Loss:5.81\n",
      "  [batch:   1000] loss: 5.75669\n",
      "  [batch:   2000] loss: 5.78315\n",
      "  [batch:   3000] loss: 5.76415\n",
      "Epoch: 30/50 Val Loss:5.913\n",
      "  [batch:   1000] loss: 5.76394\n",
      "  [batch:   2000] loss: 5.77546\n",
      "  [batch:   3000] loss: 5.74438\n",
      "Epoch: 31/50 Val Loss:5.806\n",
      "  [batch:   1000] loss: 5.77705\n",
      "  [batch:   2000] loss: 5.79647\n",
      "  [batch:   3000] loss: 5.78249\n",
      "Epoch: 32/50 Val Loss:5.84\n",
      "  [batch:   1000] loss: 5.77113\n",
      "  [batch:   2000] loss: 5.76780\n",
      "  [batch:   3000] loss: 5.77511\n",
      "Epoch: 33/50 Val Loss:5.932\n",
      "  [batch:   1000] loss: 5.76080\n",
      "  [batch:   2000] loss: 5.79463\n",
      "  [batch:   3000] loss: 5.78538\n",
      "Epoch: 34/50 Val Loss:5.863\n",
      "  [batch:   1000] loss: 5.75240\n",
      "  [batch:   2000] loss: 5.75774\n",
      "  [batch:   3000] loss: 5.77898\n",
      "Epoch: 35/50 Val Loss:5.796\n",
      "  [batch:   1000] loss: 5.75036\n",
      "  [batch:   2000] loss: 5.75675\n",
      "  [batch:   3000] loss: 5.79375\n",
      "Epoch: 36/50 Val Loss:5.815\n",
      "  [batch:   1000] loss: 5.71987\n",
      "  [batch:   2000] loss: 5.77760\n",
      "  [batch:   3000] loss: 5.76279\n",
      "Epoch: 37/50 Val Loss:6.057\n",
      "  [batch:   1000] loss: 5.75493\n",
      "  [batch:   2000] loss: 5.75988\n",
      "  [batch:   3000] loss: 5.77171\n",
      "Epoch: 38/50 Val Loss:5.826\n",
      "  [batch:   1000] loss: 5.75710\n",
      "  [batch:   2000] loss: 5.71314\n",
      "  [batch:   3000] loss: 5.75249\n",
      "Epoch: 39/50 Val Loss:5.996\n",
      "  [batch:   1000] loss: 5.78170\n",
      "  [batch:   2000] loss: 5.74211\n",
      "  [batch:   3000] loss: 5.77234\n",
      "Epoch: 40/50 Val Loss:5.792\n",
      "  [batch:   1000] loss: 5.74848\n",
      "  [batch:   2000] loss: 5.73103\n",
      "  [batch:   3000] loss: 5.78719\n",
      "Epoch: 41/50 Val Loss:5.841\n",
      "  [batch:   1000] loss: 5.72081\n",
      "  [batch:   2000] loss: 5.73512\n",
      "  [batch:   3000] loss: 5.79799\n",
      "Epoch: 42/50 Val Loss:5.805\n",
      "  [batch:   1000] loss: 5.75307\n",
      "  [batch:   2000] loss: 5.74233\n",
      "  [batch:   3000] loss: 5.76062\n",
      "Epoch: 43/50 Val Loss:5.831\n",
      "  [batch:   1000] loss: 5.76737\n",
      "  [batch:   2000] loss: 5.70806\n",
      "  [batch:   3000] loss: 5.75351\n",
      "Epoch: 44/50 Val Loss:5.835\n",
      "  [batch:   1000] loss: 5.72179\n",
      "  [batch:   2000] loss: 5.72807\n",
      "  [batch:   3000] loss: 5.71476\n",
      "Epoch: 45/50 Val Loss:5.84\n",
      "  [batch:   1000] loss: 5.73033\n",
      "  [batch:   2000] loss: 5.74337\n",
      "  [batch:   3000] loss: 5.78238\n",
      "Epoch: 46/50 Val Loss:6.093\n",
      "  [batch:   1000] loss: 5.80215\n",
      "  [batch:   2000] loss: 5.72335\n",
      "  [batch:   3000] loss: 5.71571\n",
      "Epoch: 47/50 Val Loss:5.884\n",
      "  [batch:   1000] loss: 5.70165\n",
      "  [batch:   2000] loss: 5.76429\n",
      "  [batch:   3000] loss: 5.76425\n",
      "Epoch: 48/50 Val Loss:5.816\n",
      "  [batch:   1000] loss: 5.75340\n",
      "  [batch:   2000] loss: 5.73818\n",
      "  [batch:   3000] loss: 5.75616\n",
      "Epoch: 49/50 Val Loss:5.98\n",
      "  [batch:   1000] loss: 5.75750\n",
      "  [batch:   2000] loss: 5.72657\n",
      "  [batch:   3000] loss: 5.71901\n",
      "Epoch: 50/50 Val Loss:5.832\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, sample in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        data = sample['x'].cuda()\n",
    "        target = sample['y'].cuda()\n",
    "    \n",
    "        pred = model(data)\n",
    "        loss = criterion(target, pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx and batch_idx % 1000 == 0:\n",
    "            print('  [batch:  %5d] loss: %.5f' % (batch_idx, running_loss/1000))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for val_sample in val_dataloader:\n",
    "            data = val_sample['x'].cuda()\n",
    "            target = val_sample['y'].cuda()\n",
    "            pred = model(data)\n",
    "            loss = criterion(target.float(), pred.float())\n",
    "            val_loss.append(loss.item())\n",
    "    Val_loss = round(np.mean(val_loss),3)\n",
    "    print('Epoch: ' + str(epoch+1) + '/' + str(epochs) + ' Val Loss:' + str(Val_loss))\n",
    "    if Val_loss < 6:\n",
    "        torch.save(model, \"MLP_\"+str(Val_loss)+'.pth')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "with torch.no_grad():\n",
    "    for test_sample in test_dataloader:\n",
    "        data = test_sample['x'].cuda()\n",
    "        pred = model(data)\n",
    "        pred = np.squeeze(pred).cpu().data.numpy()\n",
    "        result = np.concatenate([result, pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "cvs_output = pd.DataFrame.from_dict({'dataid': [i+1 for i in range(len(result))], 'prediction': result})\n",
    "cvs_output\n",
    "\n",
    "cvs_output.to_csv('output.csv', index = False)\n",
    "#np.savetxt('output.csv', result, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
